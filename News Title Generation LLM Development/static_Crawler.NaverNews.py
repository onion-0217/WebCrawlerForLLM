import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import time
import random
import os

# ==============================================================================
# [1] ì„¤ì • ì˜ì—­
# ==============================================================================
START_DATE = "20220101"  # ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ
END_DATE = "20251231"  # ìˆ˜ì§‘ ì¢…ë£Œ ë‚ ì§œ
OUTPUT_DIR = "naver_news_data"  # ì €ì¥í•  í´ë”ëª…

# ==============================================================================
# [2] ì´ˆê¸° ì„¤ì • ë° í—¤ë” ì¤€ë¹„
# ==============================================================================
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ì‚¬ëŒì¸ ì²™ ìœ„ì¥í•˜ëŠ” ê¸°ë³¸ í—¤ë”
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
}


# ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜
def get_date_range(start, end):
    start_dt = datetime.strptime(start, "%Y%m%d")
    end_dt = datetime.strptime(end, "%Y%m%d")
    delta = end_dt - start_dt
    return [(start_dt + timedelta(days=i)).strftime("%Y%m%d") for i in range(delta.days + 1)]


date_list = get_date_range(START_DATE, END_DATE)

# ë³€ìˆ˜ ì´ˆê¸°í™”
total_collected = 0
current_month_data = []
last_saved_month = ""
previous_url = "https://news.naver.com/"

print(f"ğŸš€ [ìµœì¢…ë³¸] í¬ë¡¤ëŸ¬ ì‹œì‘! ({START_DATE} ~ {END_DATE})")
print(f"ğŸ“‚ ë°ì´í„°ëŠ” '{OUTPUT_DIR}' í´ë”ì— ì›”ë³„ë¡œ ì €ì¥ë©ë‹ˆë‹¤.\n")

# ==============================================================================
# [3] ë©”ì¸ ìˆ˜ì§‘ ë£¨í”„
# ==============================================================================
for idx, target_date in enumerate(date_list):
    # ì •í™•í•œ ë­í‚¹ ë‰´ìŠ¤ URL
    url = f"https://news.naver.com/main/ranking/popularDay.naver?date={target_date}"

    # [í•µì‹¬] Refererë¥¼ ê³„ì† ë°”ê¿”ì„œ 'ë§í¬ë¥¼ íƒ€ê³  ë“¤ì–´ì˜¨ ì²™' ìœ„ì¥
    headers['Referer'] = previous_url

    try:
        response = requests.get(url, headers=headers, timeout=10)

        # ì ‘ì† ì‹¤íŒ¨ ì‹œ ê±´ë„ˆëœ€
        if response.status_code != 200:
            print(f"âŒ {target_date} ì ‘ì† ì‹¤íŒ¨ (Status: {response.status_code})")
            continue

        soup = BeautifulSoup(response.text, 'html.parser')

        # ----------------------------------------------------------------------
        # [4] ë°ì´í„° íŒŒì‹±
        # ----------------------------------------------------------------------
        press_boxes = soup.select('.rankingnews_box')
        daily_count = 0

        for box in press_boxes:
            try:
                # ì–¸ë¡ ì‚¬ ì´ë¦„
                press_name = box.select_one('.rankingnews_name').text.strip()

                # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
                ranks = box.select('.rankingnews_list > li')

                for rank_idx, li in enumerate(ranks):
                    # ì œëª© íƒœê·¸ ì°¾ê¸°
                    title_tag = li.select_one('.list_title')
                    if not title_tag:
                        title_tag = li.select_one('a')

                    if title_tag:
                        title = title_tag.text.strip()
                        link = title_tag.get('href')

                        current_month_data.append({
                            'date': target_date,
                            'rank': rank_idx + 1,
                            'press': press_name,
                            'title': title,
                            'link': link
                        })
                        daily_count += 1
            except Exception as e:
                continue  # íŠ¹ì • ê¸°ì‚¬ íŒŒì‹± ì—ëŸ¬ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰

        total_collected += daily_count

        # ì§„í–‰ ìƒí™© ì¶œë ¥ (Referer ë’·ë¶€ë¶„ë§Œ ë³´ì—¬ì¤Œ)
        print(f"   âœ… {target_date} (Ref: ...{previous_url[-15:]}): {daily_count}ê°œ ìˆ˜ì§‘")

        # ----------------------------------------------------------------------
        # [5] ì•ˆì „ ì¥ì¹˜ ë° ì €ì¥ ë¡œì§
        # ----------------------------------------------------------------------

        # ë‹¤ìŒ í„´ì„ ìœ„í•´ í˜„ì¬ URLì„ 'ì´ì „ URL'ë¡œ ì €ì¥
        previous_url = url

        # ëœë¤ ëŒ€ê¸° (0.5 ~ 1.5ì´ˆ)
        time.sleep(random.uniform(0.5, 1.5))

        # 30ì¼ë§ˆë‹¤ ë¸Œë¼ìš°ì € íŒ¨í„´ ë¦¬ì…‹ (ë©”ì¸ìœ¼ë¡œ ëŒì•„ê°€ëŠ” ì²™) + 5ì´ˆ íœ´ì‹
        if idx > 0 and idx % 30 == 0:
            print("   â˜• íŒ¨í„´ ë¦¬ì…‹ ë° íœ´ì‹ ì¤‘...")
            time.sleep(5)
            previous_url = "https://news.naver.com/"

        # ì›”ì´ ë°”ë€Œë©´ íŒŒì¼ ì €ì¥ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        current_month = target_date[:6]
        if last_saved_month != "" and current_month != last_saved_month:
            if current_month_data:
                save_path = f"{OUTPUT_DIR}/naver_news_{last_saved_month}.csv"
                df = pd.DataFrame(current_month_data)
                df.to_csv(save_path, index=False, encoding='utf-8-sig')
                print(f"   ğŸ’¾ [ì €ì¥] {last_saved_month} ë°ì´í„° ì €ì¥ ì™„ë£Œ ({len(df)}ê±´)")
                current_month_data = []  # ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

        last_saved_month = current_month

    except Exception as e:
        print(f"   âš ï¸ ì—ëŸ¬ ë°œìƒ ({target_date}): {e}")
        time.sleep(10)  # ì—ëŸ¬ë‚˜ë©´ 10ì´ˆ ëŒ€ê¸°

# ==============================================================================
# [6] ë‚¨ì€ ë°ì´í„° ìµœì¢… ì €ì¥
# ==============================================================================
if current_month_data:
    save_path = f"{OUTPUT_DIR}/naver_news_{last_saved_month}.csv"
    df = pd.DataFrame(current_month_data)
    df.to_csv(save_path, index=False, encoding='utf-8-sig')
    print(f"   ğŸ’¾ [ìµœì¢… ì €ì¥] {last_saved_month} ë°ì´í„° ì €ì¥ ì™„ë£Œ")

print("\n" + "=" * 50)
print(f"ğŸ‰ ëª¨ë“  ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ì´ ìˆ˜ì§‘ëœ ë°ì´í„°: {total_collected}ê±´")
print("=" * 50)