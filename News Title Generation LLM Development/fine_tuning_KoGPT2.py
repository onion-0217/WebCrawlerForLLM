import torch
from torch.utils.data import Dataset, DataLoader
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
from torch.optim import AdamW
import os
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œë°”

# ==========================================
# 1. ì„¤ì •
# ==========================================
EPOCHS = 3  # í•™ìŠµ íšŸìˆ˜ (íšŸìˆ˜ê°€ ëŠ˜ì–´ë‚˜ë©´ ê³¼ì í•© ë¬¸ì œ ë°œìƒ)
BATCH_SIZE = 8  # í•œ ë²ˆì— ê³µë¶€í•  ë¬¸ì œ ìˆ˜
LEARNING_RATE = 3e-5  # í•™ìŠµ ì†ë„
MAX_LEN = 64  # ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ (ì œëª©ì´ë‹ˆê¹Œ ì§§ê²Œ 64ë¡œ ì„¤ì •)

DATA_PATH = "train_style_data.txt"
MODEL_NAME = "skt/kogpt2-base-v2"
OUTPUT_DIR = "news_title_model"  # í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë  í´ë”

# GPUê°€ ìˆìœ¼ë©´ ì“°ê³ , ì—†ìœ¼ë©´ CPUë¥¼ ì”ë‹ˆë‹¤
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¥ í•™ìŠµ ì¥ì¹˜: {device}")


# ==========================================
# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ë°ì´í„°ë¥¼ AIì—ê²Œ ë– ë¨¹ì—¬ì£¼ëŠ” ìˆŸê°€ë½)
# ==========================================
class NewsTitleDataset(Dataset):
    def __init__(self, file_path, tokenizer, max_len):
        self.data = []
        self.tokenizer = tokenizer
        self.max_len = max_len

        print("ğŸ“‚ ë°ì´í„°ë¥¼ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        for line in tqdm(lines):
            line = line.strip()
            if not line: continue

            # í† í¬ë‚˜ì´ì§• (ê¸€ìë¥¼ ìˆ«ìë¡œ ë³€í™˜)
            tokenized = tokenizer(
                line,
                padding="max_length",
                truncation=True,
                max_length=max_len,
                return_tensors="pt"
            )

            self.data.append({
                "input_ids": tokenized["input_ids"][0],
                "attention_mask": tokenized["attention_mask"][0]
            })

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


# ==========================================
# 3. í•™ìŠµ ì‹¤í–‰ (ë©”ì¸ ë¡œì§)
# ==========================================
def main():
    # í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (SKT KoGPT2)
    tokenizer = PreTrainedTokenizerFast.from_pretrained(
        MODEL_NAME,
        bos_token='</s>',
        eos_token='</s>',
        unk_token='<unk>',
        pad_token='<pad>',
        mask_token='<mask>'
    )

    model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)
    model.to(device)
    model.train()  # í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜

    # ë°ì´í„° ë¡œë” ì¤€ë¹„
    dataset = NewsTitleDataset(DATA_PATH, tokenizer, MAX_LEN)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    # ìµœì í™” ë„êµ¬ (Optimizer) ì„¤ì •
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

    print(f"\nğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤! (ì´ {len(dataset)}ê°œ ë¬¸ì¥, {EPOCHS} ì—í­)")

    for epoch in range(EPOCHS):
        total_loss = 0
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{EPOCHS}")

        for batch in progress_bar:
            optimizer.zero_grad()  # ì´ì „ ê¸°ìš¸ê¸° ì´ˆê¸°í™”

            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            # ëª¨ë¸ì—ê²Œ ì •ë‹µì§€(labels)ë¥¼ ì£¼ë©´ ì•Œì•„ì„œ lossë¥¼ ê³„ì‚°í•¨
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=input_ids
            )

            loss = outputs.loss
            loss.backward()  # ì—­ì „íŒŒ (ì˜¤ë‹µë…¸íŠ¸ ì‘ì„±)
            optimizer.step()  # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ê³µë¶€ ë‚´ìš© ë°˜ì˜)

            total_loss += loss.item()
            progress_bar.set_postfix({'loss': f"{loss.item():.4f}"})

        avg_loss = total_loss / len(dataloader)
        print(f"ğŸ“Š Epoch {epoch + 1} ì¢…ë£Œ - í‰ê·  Loss: {avg_loss:.4f}")

        # ì—í­ë§ˆë‹¤ ëª¨ë¸ ì €ì¥
        model.save_pretrained(f"{OUTPUT_DIR}/checkpoint-{epoch + 1}")
        tokenizer.save_pretrained(f"{OUTPUT_DIR}/checkpoint-{epoch + 1}")

    # ìµœì¢… ì €ì¥
    print("\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"ğŸ‰ í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ '{OUTPUT_DIR}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    if not os.path.exists(DATA_PATH):
        print("âŒ í•™ìŠµ ë°ì´í„° íŒŒì¼(train_style_data.txt)ì´ ì—†ìŠµë‹ˆë‹¤!")
    else:
        main()